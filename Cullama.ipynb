{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_lhpLaijTc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/customLlama/blob/main/Cullama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_A8ONRGk_--"
      },
      "source": [
        "### 0 - Pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqrsEoOuijTe"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub transformers trl peft bitsandbytes datasets openpyxl xlrd protobuf safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3VPw7SfijTf"
      },
      "source": [
        "### 1 - Log in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eenc8yVijTf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token = \"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\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIOBVujijTg"
      },
      "source": [
        "### 2 - Download pretrained model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name        = \"yentinglin/Llama-3-Taiwan-8B-Instruct\" # \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "num_hidden_layers = 2\n",
        "\n",
        "config = LlamaConfig.from_pretrained(model_name, cache_dir=\"./\")\n",
        "config.num_hidden_layers = num_hidden_layers\n",
        "new_model      = LlamaForCausalLM(config)\n",
        "new_state_dict = {}\n",
        "\n",
        "old_model      = LlamaForCausalLM.from_pretrained(model_name, cache_dir=\"./\")\n",
        "old_state_dict = old_model.state_dict()\n",
        "\n",
        "for name, param in new_model.state_dict().items():\n",
        "    if name in old_state_dict:\n",
        "        if 'layers.' in name:\n",
        "            # Check if the layer index is within the range of the new model\n",
        "            original_layer_index = int(name.split('.')[2])\n",
        "            if original_layer_index < config.num_hidden_layers:\n",
        "                new_state_dict[name] = old_state_dict[name]\n",
        "        else:\n",
        "            new_state_dict[name] = old_state_dict[name]\n",
        "\n",
        "# Load the filtered state dictionary into the new model\n",
        "new_model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "model = new_model\n",
        "\n",
        "tokenizer   = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir=\"./\" )\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSdL_Gjv1-"
      },
      "source": [
        "#### *** Check layers, modules, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhvqrp8Bjfug"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKlBha1ijTg"
      },
      "source": [
        "### 3 - Customize your attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 - Append your attn to llama's attn"
      ],
      "metadata": {
        "id": "WPbGLhCgl2Nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBtimJjEijTg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 - Append traditional attn to llama's attn"
      ],
      "metadata": {
        "id": "8MZ-44VdmJ9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Op8EpDIoYR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ],
      "metadata": {
        "id": "SThX_SkomAtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ABgPNYHlS420"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 - Replace llama's attn with your attn (consecutive softmax)"
      ],
      "metadata": {
        "id": "2we5nrMyxLrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1YWyXQiGxLe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNNZV_KYIoYR"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2lmCqhkIoYR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoIoiiQ6ijTh"
      },
      "outputs": [],
      "source": [
        "# from peft import get_peft_model, LoraConfig, TaskType\n",
        "#\n",
        "# # Define LoRA configuration\n",
        "# lora_config = LoraConfig(\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False,\n",
        "#     r=100,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.1,\n",
        "# )\n",
        "#\n",
        "# # Apply LoRA to the quantized model\n",
        "# model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0_TEzKPIoYR"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   print(model.model.layers[0])\n",
        "#   print(model.model.layers[0].self_attn.state_dict())\n",
        "# except:\n",
        "#   print(model.base_model.model.model.layers[0])\n",
        "#   print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "#\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters())\n",
        "# num_params = count_parameters(model)\n",
        "# print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nnjoE9kijTh"
      },
      "source": [
        "### 4 - Prep data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGACt7bfIoYS"
      },
      "source": [
        "#### 4.1 Use HF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-Q9adqhIoYS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define the prompt and EOS token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### instruction:\n",
        "{}\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### output:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Function to format the dataset into prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Load the dataset\n",
        "# ikala/tmmluplus\n",
        "# kigner/ruozhiba-llama3\n",
        "dataset = load_dataset(\"kigner/ruozhiba-llama3\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "train_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare labels (same as input_ids)\n",
        "def prepare_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "train_dataset = train_dataset.map(prepare_labels, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFj8QufiIoYS"
      },
      "source": [
        "#### 4.2 Use local data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PonguRS8IoYS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define the prompt and EOS token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### instruction:\n",
        "{}\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### output:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "# Function to format the dataset into prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Read local xlsx\n",
        "import pandas as pd\n",
        "excel_file = pd.ExcelFile('./fintech_qa.xlsx', engine='openpyxl')\n",
        "for sheet_name in excel_file.sheet_names:\n",
        "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "    df.to_csv(f'./{sheet_name}.csv', index=False)\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files={'train': f'./{sheet_name}.csv'})\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)['train']\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "train_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare labels (same as input_ids)\n",
        "def prepare_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "train_dataset = train_dataset.map(prepare_labels, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpvJyF9ijTh"
      },
      "source": [
        "### 5 - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtcV0l2DijTh"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    num_train_epochs=100,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy='no'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcT8AgMLIoYT"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmHws9zIoYT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ng5N-o6ijTh"
      },
      "source": [
        "### 6 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wYrzVOSijTh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_response(input_text, model, tokenizer, max_length=100):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_k=50,  # Optional: for more diverse output\n",
        "            top_p=0.95  # Optional: for more diverse output\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = \"金控負責人以個人名義兼海外，是否要董事會決議?\"\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0QogOOIoYT"
      },
      "source": [
        "### 7 - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPFr9F1CijTh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define the prompt and EOS token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### instruction:\n",
        "{}\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### output:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Function to format the dataset into prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"kigner/ruozhiba-llama3\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "test_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare labels (same as input_ids)\n",
        "def prepare_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "test_dataset = test_dataset.map(prepare_labels, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
        "\n",
        "# Now test_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "testing_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=1\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=testing_args,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "trainer_stats = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(trainer_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJvLx7CijTi"
      },
      "source": [
        "### 8 - Save & Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqZX2ggwijTi"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"Llama-3-Taiwan-8B-Instruct-to-1B\")\n",
        "tokenizer.save_pretrained(\"Llama-3-Taiwan-8B-Instruct-to-1B\")\n",
        "model.push_to_hub(\"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\", token = \"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\") # Online saving\n",
        "tokenizer.push_to_hub(\"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\", token = \"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\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcujtpQYijTi"
      },
      "source": [
        "### 9- Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjHEq5G0ijTi"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\" # \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BYhx2cqIoYW"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj2xX5qAIoYW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10 - Customize your attention"
      ],
      "metadata": {
        "id": "Y6THnZmPPWj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10-1 Append your attn to llama's attn"
      ],
      "metadata": {
        "id": "RUiTgRsno6fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  position_embeddings=position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nTYXL0WBP400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.2 - Append traditional attn to llama's attn"
      ],
      "metadata": {
        "id": "Mv7Bfuo6pFd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ],
      "metadata": {
        "id": "-UD52W8ZpJz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ],
      "metadata": {
        "id": "gOA0BFmCozhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JBonirDSoxzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.4 - Replace llama's attn with your attn (consecutive softmax)"
      ],
      "metadata": {
        "id": "8sHaB6c-yIj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4fK7x-qKyxVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *** Check"
      ],
      "metadata": {
        "id": "4XmngNrDP2Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ],
      "metadata": {
        "id": "EgxI0DkCP4AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11 - Retreive safetensor"
      ],
      "metadata": {
        "id": "rKmDSBFCQZkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Replace with your model's repository ID\n",
        "repo_id = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"  # Example: \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"\n",
        "\n",
        "# Specify the filenames of the SafeTensor files you want to download\n",
        "filenames = [\"model-00001-of-00002.safetensors\", \"model-00002-of-00002.safetensors\"]\n",
        "\n",
        "# Specify dir\n",
        "custom_cache_dir =  \"./\"\n",
        "\n",
        "# Download each SafeTensor file\n",
        "for filename in filenames:\n",
        "    filepath = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=custom_cache_dir, token = \"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\")\n",
        "\n",
        "\n",
        "\n",
        "state_dict_combined = {}\n",
        "\n",
        "# Iterate over all safetensor files in the directory\n",
        "for filename in os.listdir(os.path.join(custom_cache_dir, \"models--Brownwang0426--Llama-3-Taiwan-8B-Instruct-to-1B\" ,\"blobs\")):\n",
        "    file_path = os.path.join(custom_cache_dir, \"models--Brownwang0426--Llama-3-Taiwan-8B-Instruct-to-1B\" ,\"blobs\", filename)\n",
        "    print(f\"Loading {file_path}...\")\n",
        "\n",
        "    # Load the safetensor file\n",
        "    state_dict = load_file(file_path)\n",
        "\n",
        "    # Merge the state_dict with the merged_state_dict\n",
        "    state_dict_combined.update(state_dict)\n",
        "\n",
        "# Merge the weights from the current SafeTensor\n",
        "for key, tensor in state_dict_combined.items():\n",
        "    if key in model.state_dict():\n",
        "        print(f\"Merging {key}.\")\n",
        "        model.state_dict()[key].copy_(tensor)\n",
        "    else:\n",
        "        print(f\"Warning: {key} not found in the model. Skipping.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bvAk4n9WQhUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *** Check"
      ],
      "metadata": {
        "id": "Lz47c6eEQXYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ],
      "metadata": {
        "id": "0H_0V6InQhrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdqZDV7EijTi"
      },
      "source": [
        "### 12 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKnSFq0_ijTi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_response(input_text, model, tokenizer, max_length=100):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_k=50,  # Optional: for more diverse output\n",
        "            top_p=0.95  # Optional: for more diverse output\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = \"金控負責人以個人名義兼海外，是否要董事會決議?\"\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqvoePw5OQwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYnZlfg4OQtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEfD3VLJOQqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sy64sEj_OQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13DOWEMYOQhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQbHaYnFijTi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9F3UqXtijTi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
