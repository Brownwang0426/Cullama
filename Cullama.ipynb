{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_lhpLaijTc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/customLlama/blob/main/Cullama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_A8ONRGk_--"
      },
      "source": [
        "### 0 - Pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tqrsEoOuijTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8015cfb1-7b00-47a8-de7d-ca539ec45ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub==0.24.5\n",
            "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting transformers==4.44.0\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl==0.9.6\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting peft==0.12.0\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting bitsandbytes==0.43.3\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting datasets==2.20.0\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: xlrd==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Collecting protobuf==5.27.3\n",
            "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: safetensors==0.4.4 in /usr/local/lib/python3.10/dist-packages (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.5) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.0) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.0) (0.19.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.9.6) (2.3.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.9.6) (0.32.1)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
            "  Downloading tyro-0.8.8-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (5.9.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets==2.20.0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.1.4)\n",
            "Collecting xxhash (from datasets==2.20.0)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.24.5)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.10.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl==3.1.5) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.5) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.9.6)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.9.6) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.6)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
            "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading tyro-0.8.8-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: xxhash, shtab, pyarrow, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface_hub, tyro, nvidia-cusolver-cu12, transformers, datasets, bitsandbytes, trl, peft\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.23.5\n",
            "    Uninstalling huggingface-hub-0.23.5:\n",
            "      Successfully uninstalled huggingface-hub-0.23.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-aiplatform 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.43.3 datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 huggingface_hub-0.24.5 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 peft-0.12.0 protobuf-5.27.3 pyarrow-17.0.0 shtab-1.7.1 transformers-4.44.0 trl-0.9.6 tyro-0.8.8 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.24.5 transformers==4.44.0 trl==0.9.6 peft==0.12.0 bitsandbytes==0.43.3 datasets==2.20.0 openpyxl==3.1.5 xlrd==2.0.1 protobuf==5.27.3 safetensors==0.4.4 faiss-cpu==1.8.0 langchain==0.2.14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3VPw7SfijTf"
      },
      "source": [
        "### 1 - Log in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--U3U8mVo_2Q"
      },
      "outputs": [],
      "source": [
        "your_huggingface_finegrained_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Eenc8yVijTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6ec115-a352-4908-d325-c04892a840f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token = your_huggingface_finegrained_token )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIOBVujijTg"
      },
      "source": [
        "### 2 - Download pretrained model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIOBgOno_2S"
      },
      "source": [
        "Select your llama-3 model and the number of layers you want to preserve from huggingface. We support only llama-3 for the present time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHGPoCrzo_2T"
      },
      "outputs": [],
      "source": [
        "your_model_name              = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
        "your_preserved_hidden_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "if your_preserved_hidden_layers != \"all\":\n",
        "    model_name        = your_model_name\n",
        "    num_hidden_layers = your_preserved_hidden_layers\n",
        "\n",
        "    config            = LlamaConfig.from_pretrained(model_name)\n",
        "    config.num_hidden_layers = num_hidden_layers\n",
        "    new_model         = LlamaForCausalLM(config)\n",
        "    new_state_dict    = {}\n",
        "\n",
        "    old_model         = LlamaForCausalLM.from_pretrained(model_name)\n",
        "    old_state_dict    = old_model.state_dict()\n",
        "\n",
        "    for name, param in new_model.state_dict().items():\n",
        "        if name in old_state_dict:\n",
        "            if 'layers.' in name:\n",
        "                # Check if the layer index is within the range of the new model\n",
        "                original_layer_index = int(name.split('.')[2])\n",
        "                if original_layer_index < config.num_hidden_layers:\n",
        "                    new_state_dict[name] = old_state_dict[name]\n",
        "            else:\n",
        "                new_state_dict[name] = old_state_dict[name]\n",
        "\n",
        "    # Load the filtered state dictionary into the new model\n",
        "    new_model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    model = new_model\n",
        "\n",
        "    tokenizer   = PreTrainedTokenizerFast.from_pretrained(model_name )\n",
        "\n",
        "    # if tokenizer.pad_token is None:\n",
        "    #     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    del old_model\n",
        "\n",
        "else:\n",
        "\n",
        "    model       = LlamaForCausalLM.from_pretrained(your_model_name)\n",
        "    tokenizer   = PreTrainedTokenizerFast.from_pretrained(your_model_name )\n",
        "\n",
        "    # if tokenizer.pad_token is None:\n",
        "    #     tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSdL_Gjv1-"
      },
      "source": [
        "#### *** Check layers, modules, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhvqrp8Bjfug"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKlBha1ijTg"
      },
      "source": [
        "### 3 - Customize your attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPbGLhCgl2Nw"
      },
      "source": [
        "#### 3.1 - Append your attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBtimJjEijTg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MZ-44VdmJ9O"
      },
      "source": [
        "#### 3.2 - Append traditional attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Op8EpDIoYR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SThX_SkomAtl"
      },
      "source": [
        "#### 3.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgPNYHlS420"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2we5nrMyxLrl"
      },
      "source": [
        "#### 3.4 - Replace llama's attn with your attn (consecutive softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWyXQiGxLe-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m30u0pOo_2X"
      },
      "source": [
        "#### 3.5 - No Customization! Only fine-tune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RgrDl9Oo_2X"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHt8IZSNo_2X"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"] # I don't know why these parameters are called \"modules\"... They should be \"parameters\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yhrf8gto_2X"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank decomposition\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    target_modules= target_modules,  # Target modules to apply LoRA\n",
        "    bias=\"none\"  # Bias handling, can be \"none\", \"lora_only\", or \"both\"\n",
        ")\n",
        "\n",
        "model       = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_dqEL4Zo_2X"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqQ7oD3Ho_2X"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nnjoE9kijTh"
      },
      "source": [
        "### 4 - Prep data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGACt7bfIoYS"
      },
      "source": [
        "#### 4.1 Use HF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kpt8KXM8o_2Y"
      },
      "outputs": [],
      "source": [
        "your_train_dataset = \"taide/TAIDE-14-tasks\" # Jamie0510/taiwan-law-exam\n",
        "split              = \"train\"\n",
        "sub_set            = ''\n",
        "instruction_col    = 'Prompt'\n",
        "input_col          = 'Input'\n",
        "output_col         = 'Positive Response'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X-Q9adqhIoYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3aa2f1-e555-4596-ebfa-23ba6ad0cfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'Topic', 'Task', 'Keywords', 'Prompt', 'Input', 'Positive Response', 'Negative Response']\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 140\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = load_dataset(your_train_dataset, sub_set)[split]\n",
        "original_cols = train_dataset.column_names\n",
        "print(original_cols)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples[instruction_col], examples[input_col] )]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs              , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples[output_col], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns(original_cols)\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFj8QufiIoYS"
      },
      "source": [
        "#### 4.2 Use local data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mRpEYLEo_2Y"
      },
      "outputs": [],
      "source": [
        "your_train_dataset = './fintech_qa.xlsx'\n",
        "split              = \"train\"\n",
        "instruction_col    = 'instruction'\n",
        "input_col          = 'input'\n",
        "output_col         = 'output'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PonguRS8IoYS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Read local xlsx\n",
        "import pandas as pd\n",
        "excel_file = pd.ExcelFile(your_train_dataset, engine='openpyxl')\n",
        "for sheet_name in excel_file.sheet_names:\n",
        "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "    df.to_csv(f'./{sheet_name}.csv', index=False)\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset('csv', data_files={'train': f'./{sheet_name}.csv'})[split]\n",
        "original_cols = train_dataset.column_names\n",
        "print(original_cols)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples[instruction_col], examples[input_col])]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs              , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples[output_col], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns(original_cols)\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpvJyF9ijTh"
      },
      "source": [
        "### 5 - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qqv3Hyhso_2Z"
      },
      "outputs": [],
      "source": [
        "your_learning_rate = 1e-4\n",
        "your_batch_size    = 1\n",
        "your_epoch         = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HtcV0l2DijTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "1aab40e3-b197-402f-ebcb-0ca58b64e44c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-350f8b681ece>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1949\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2349\u001b[0m                             \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_xla_gradients_synced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 )\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    125\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU "
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './result',\n",
        "    learning_rate= your_learning_rate,\n",
        "    per_device_train_batch_size=your_batch_size,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    num_train_epochs=your_epoch,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=  \"no\" # 'epoch',  # Save model every epoch\n",
        "    # save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcT8AgMLIoYT"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmHws9zIoYT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ng5N-o6ijTh"
      },
      "source": [
        "### 6 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOebwXdko_2d"
      },
      "outputs": [],
      "source": [
        "your_user_input = \"請問你為何外匯帳戶容易受到洗錢者的喜愛?拜託不要跟我講幹話啦\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fjMWEeo_2d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "def generate_response(input_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt').to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_length=len(inputs[\"input_ids\"][0]) + 100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.5,\n",
        "            do_sample=True,\n",
        "            top_k=50,   # Optional: for more diverse output\n",
        "            top_p=0.95      # Optional: for more diverse output\n",
        "        )\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = your_user_input\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response.replace(your_user_input, \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0QogOOIoYT"
      },
      "source": [
        "### 7 - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNIBC7gdo_2d"
      },
      "outputs": [],
      "source": [
        "your_test_dataset = \"kigner/ruozhiba-llama3\"\n",
        "split = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPFr9F1CijTh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "test_dataset = load_dataset(your_test_dataset)[split]\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples['instruction'], examples['input'])]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs            , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples['output'], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "testing_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=1\n",
        ")\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=testing_args,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "trainer_stats = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(trainer_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJvLx7CijTi"
      },
      "source": [
        "### 8 - Save & Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulcghG4bo_2d"
      },
      "outputs": [],
      "source": [
        "your_repo_name  = \"Brownwang0426\"\n",
        "your_model_name = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"\n",
        "your_huggingface_write_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqZX2ggwijTi"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(your_model_name)\n",
        "tokenizer.save_pretrained(your_model_name)\n",
        "model.push_to_hub(your_model_name, token = your_huggingface_write_token) # Online saving\n",
        "tokenizer.push_to_hub(your_model_name, token = your_huggingface_write_token) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcujtpQYijTi"
      },
      "source": [
        "### 9- Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eTd-6BUWo_2e"
      },
      "outputs": [],
      "source": [
        "your_model_name              =  \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MjHEq5G0ijTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "9e307693c07c4c4381ddda2e52d7de00",
            "4289d9586f2444d19dde8a36d2aa8551",
            "2bf29d159c9e447bae2d012d6bf32a55",
            "5bc86a4ce084452c84a975d71efc2734",
            "7846bff184604650ae189a8aef9fc6fe",
            "e82bd924f4ba4f47bc98174ee865beee",
            "496f7e5650724f43944885dd5e5f0116",
            "283ded45f7294c8e984ef42e3ed8d2dc",
            "31a5715b108644e5a2177e71ca1461c0",
            "68a4195009b6481486dba2b4d9e75bd3",
            "93fe7e9e047047fd99798d2207feaa81"
          ]
        },
        "outputId": "af1a32d7-8ba8-40d4-fd33-2dd0ec4654a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e307693c07c4c4381ddda2e52d7de00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.custom_attn.W_k_0.weight', 'model.layers.0.self_attn.custom_attn.W_k_1.weight', 'model.layers.0.self_attn.custom_attn.W_k_2.weight', 'model.layers.0.self_attn.custom_attn.W_k_final.weight', 'model.layers.0.self_attn.custom_attn.W_o_0.weight', 'model.layers.0.self_attn.custom_attn.W_o_1.weight', 'model.layers.0.self_attn.custom_attn.W_o_2.weight', 'model.layers.0.self_attn.custom_attn.W_o_final.weight', 'model.layers.0.self_attn.custom_attn.W_q_0.weight', 'model.layers.0.self_attn.custom_attn.W_q_1.weight', 'model.layers.0.self_attn.custom_attn.W_q_2.weight', 'model.layers.0.self_attn.custom_attn.W_q_final.weight', 'model.layers.0.self_attn.custom_attn.W_v_0.weight', 'model.layers.0.self_attn.custom_attn.W_v_1.weight', 'model.layers.0.self_attn.custom_attn.W_v_2.weight', 'model.layers.0.self_attn.custom_attn.W_v_final.weight', 'model.layers.1.self_attn.custom_attn.W_k_0.weight', 'model.layers.1.self_attn.custom_attn.W_k_1.weight', 'model.layers.1.self_attn.custom_attn.W_k_2.weight', 'model.layers.1.self_attn.custom_attn.W_k_final.weight', 'model.layers.1.self_attn.custom_attn.W_o_0.weight', 'model.layers.1.self_attn.custom_attn.W_o_1.weight', 'model.layers.1.self_attn.custom_attn.W_o_2.weight', 'model.layers.1.self_attn.custom_attn.W_o_final.weight', 'model.layers.1.self_attn.custom_attn.W_q_0.weight', 'model.layers.1.self_attn.custom_attn.W_q_1.weight', 'model.layers.1.self_attn.custom_attn.W_q_2.weight', 'model.layers.1.self_attn.custom_attn.W_q_final.weight', 'model.layers.1.self_attn.custom_attn.W_v_0.weight', 'model.layers.1.self_attn.custom_attn.W_v_1.weight', 'model.layers.1.self_attn.custom_attn.W_v_2.weight', 'model.layers.1.self_attn.custom_attn.W_v_final.weight']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B and are newly initialized: ['model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model_name = your_model_name # \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BYhx2cqIoYW"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj2xX5qAIoYW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6THnZmPPWj6"
      },
      "source": [
        "### 10 - Customize your attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUiTgRsno6fR"
      },
      "source": [
        "#### 10.1 - Append your attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTYXL0WBP400"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  position_embeddings=position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv7Bfuo6pFd-"
      },
      "source": [
        "#### 10.2 - Append traditional attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UD52W8ZpJz6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOA0BFmCozhP"
      },
      "source": [
        "#### 10.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JBonirDSoxzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dc8ab2-a7c7-45eb-facc-8dd9bd463483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing attention layer: 0\n",
            "Replacing attention layer: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sHaB6c-yIj1"
      },
      "source": [
        "#### 10.4 - Replace llama's attn with your attn (consecutive softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fK7x-qKyxVW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3gmnrVqo_2g"
      },
      "source": [
        "#### 10.5 - No Customization! Only fine-tune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqi3qqfvo_2g"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sL0nTrFo_2g"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"] # I don't know why these parameters are called \"modules\"... They should be \"parameters\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3XXjWkvo_2g"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank decomposition\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    target_modules= target_modules,  # Target modules to apply LoRA\n",
        "    bias=\"none\"  # Bias handling, can be \"none\", \"lora_only\", or \"both\"\n",
        ")\n",
        "\n",
        "model       = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XmngNrDP2Zo"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgxI0DkCP4AC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKmDSBFCQZkb"
      },
      "source": [
        "### 11 - Retreive safetensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZAT5CCD-o_2g"
      },
      "outputs": [],
      "source": [
        "your_model_name  = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"\n",
        "your_safetensors = [\"model-00001-of-00002.safetensors\", \"model-00002-of-00002.safetensors\"]\n",
        "your_huggingface_read_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bvAk4n9WQhUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49b09a9-1862-4c48-9396-d94851e88144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ./models--Brownwang0426--Llama-3-Taiwan-8B-Instruct-to-1B/blobs/9f6e706b9e91d5614b3b8d1a6eb9e1a6dc4890e62fdbbed613ed814725c5a3af...\n",
            "Loading ./models--Brownwang0426--Llama-3-Taiwan-8B-Instruct-to-1B/blobs/c4e05a23c1a1166bcf547324e8e500acee1eaff5949fc390f3bf15b16c1d549c...\n",
            "Merging model.embed_tokens.weight.\n",
            "Merging model.layers.0.input_layernorm.weight.\n",
            "Merging model.layers.0.mlp.down_proj.weight.\n",
            "Merging model.layers.0.mlp.gate_proj.weight.\n",
            "Merging model.layers.0.mlp.up_proj.weight.\n",
            "Merging model.layers.0.post_attention_layernorm.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_k_0.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_k_1.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_k_2.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_k_final.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_o_0.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_o_1.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_o_2.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_o_final.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_q_0.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_q_1.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_q_2.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_q_final.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_v_0.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_v_1.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_v_2.weight.\n",
            "Merging model.layers.0.self_attn.custom_attn.W_v_final.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_k_0.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_k_1.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_k_2.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_k_final.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_o_0.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_o_1.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_o_2.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_o_final.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_q_0.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_q_1.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_q_2.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_q_final.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_v_0.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_v_1.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_v_2.weight.\n",
            "Merging model.layers.1.self_attn.custom_attn.W_v_final.weight.\n",
            "Merging lm_head.weight.\n",
            "Merging model.layers.1.input_layernorm.weight.\n",
            "Merging model.layers.1.mlp.down_proj.weight.\n",
            "Merging model.layers.1.mlp.gate_proj.weight.\n",
            "Merging model.layers.1.mlp.up_proj.weight.\n",
            "Merging model.layers.1.post_attention_layernorm.weight.\n",
            "Merging model.norm.weight.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Replace with your model's repository ID\n",
        "repo_id = your_model_name\n",
        "\n",
        "# Specify the filenames of the SafeTensor files you want to download\n",
        "filenames = your_safetensors\n",
        "\n",
        "# Specify dir\n",
        "custom_cache_dir =  \"./\"\n",
        "\n",
        "# Download each SafeTensor file\n",
        "for filename in filenames:\n",
        "    try:\n",
        "        filepath = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=custom_cache_dir, token = your_huggingface_read_token )\n",
        "    except:\n",
        "        print(f'safetenstor not found: {filename}')\n",
        "\n",
        "state_dict_combined = {}\n",
        "\n",
        "# Iterate over all safetensor files in the directory\n",
        "for filename in os.listdir(os.path.join(custom_cache_dir, \"models--\" + your_model_name.replace(\"/\", \"--\") ,\"blobs\")):\n",
        "    file_path = os.path.join(custom_cache_dir, \"models--\" + your_model_name.replace(\"/\", \"--\") ,\"blobs\", filename)\n",
        "    print(f\"Loading {file_path}...\")\n",
        "\n",
        "    # Load the safetensor file\n",
        "    state_dict = load_file(file_path)\n",
        "\n",
        "    # Merge the state_dict with the merged_state_dict\n",
        "    state_dict_combined.update(state_dict)\n",
        "\n",
        "# Merge the weights from the current SafeTensor\n",
        "for key, tensor in state_dict_combined.items():\n",
        "    if key in model.state_dict():\n",
        "        print(f\"Merging {key}.\")\n",
        "        model.state_dict()[key].copy_(tensor)\n",
        "    else:\n",
        "        print(f\"Warning: {key} not found in the model. Skipping.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz47c6eEQXYp"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H_0V6InQhrm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdqZDV7EijTi"
      },
      "source": [
        "### 12 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7KGw2cMYo_2h"
      },
      "outputs": [],
      "source": [
        "your_user_input = \"你是誰\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TKnSFq0_ijTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e0382d-2876-4d84-dff2-af594faa6d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: 約新台幣1, paired with an input:\n",
            "人從事放款業務，未辦法規，經主管機關….(三)單一)第4條：\n",
            "一事件罰鍰金控股公司法規：證券上市公司，是否要發重訊?(111/07/07/06/07/24)\n",
            "\n",
            "### output:\n",
            "說明：\n",
            "一銀行法規：…因違反金融控股公司法規：證\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "def generate_response(input_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt').to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_length=len(inputs[\"input_ids\"][0]) + 100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.5,\n",
        "            do_sample=True,\n",
        "            top_k=50,   # Optional: for more diverse output\n",
        "            top_p=0.95      # Optional: for more diverse output\n",
        "        )\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = your_user_input\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response.replace(your_user_input, \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13 - Let's play with RAG with this tiny model!"
      ],
      "metadata": {
        "id": "jfo-q_mizw_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HqvoePw5OQwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "2bb76fc5-3831-4556-b311-3ac059af4bfb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'InMemoryRetriever' from 'langchain.retrievers' (/usr/local/lib/python3.10/dist-packages/langchain/retrievers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3e8e9340ee71>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrievers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryRetriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m retriever = InMemoryRetriever(documents=[\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Document 1 content\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Document 2 content\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'InMemoryRetriever' from 'langchain.retrievers' (/usr/local/lib/python3.10/dist-packages/langchain/retrievers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.retrievers import InMemoryRetriever\n",
        "\n",
        "retriever = InMemoryRetriever(documents=[\n",
        "    {\"text\": \"Document 1 content\"},\n",
        "    {\"text\": \"Document 2 content\"}\n",
        "    # Add more documents as needed\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BYnZlfg4OQtS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f5650a78-ea47-44ba-f28d-778d4a9ddd82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'RAG' from 'langchain.chains' (/usr/local/lib/python3.10/dist-packages/langchain/chains/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b0620988f76c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m rag_chain = RAG(\n\u001b[1;32m      4\u001b[0m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'RAG' from 'langchain.chains' (/usr/local/lib/python3.10/dist-packages/langchain/chains/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.chains import RAG\n",
        "\n",
        "rag_chain = RAG(\n",
        "    retriever=retriever,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_output=False  # Set to True if you want detailed output\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy64sEj_OQoK"
      },
      "outputs": [],
      "source": [
        "query = \"Your query here\"\n",
        "result = rag_chain.run(query)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13DOWEMYOQhP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQbHaYnFijTi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9F3UqXtijTi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e307693c07c4c4381ddda2e52d7de00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4289d9586f2444d19dde8a36d2aa8551",
              "IPY_MODEL_2bf29d159c9e447bae2d012d6bf32a55",
              "IPY_MODEL_5bc86a4ce084452c84a975d71efc2734"
            ],
            "layout": "IPY_MODEL_7846bff184604650ae189a8aef9fc6fe"
          }
        },
        "4289d9586f2444d19dde8a36d2aa8551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e82bd924f4ba4f47bc98174ee865beee",
            "placeholder": "​",
            "style": "IPY_MODEL_496f7e5650724f43944885dd5e5f0116",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2bf29d159c9e447bae2d012d6bf32a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_283ded45f7294c8e984ef42e3ed8d2dc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31a5715b108644e5a2177e71ca1461c0",
            "value": 2
          }
        },
        "5bc86a4ce084452c84a975d71efc2734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a4195009b6481486dba2b4d9e75bd3",
            "placeholder": "​",
            "style": "IPY_MODEL_93fe7e9e047047fd99798d2207feaa81",
            "value": " 2/2 [00:00&lt;00:00,  4.13it/s]"
          }
        },
        "7846bff184604650ae189a8aef9fc6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82bd924f4ba4f47bc98174ee865beee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496f7e5650724f43944885dd5e5f0116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283ded45f7294c8e984ef42e3ed8d2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a5715b108644e5a2177e71ca1461c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68a4195009b6481486dba2b4d9e75bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93fe7e9e047047fd99798d2207feaa81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}