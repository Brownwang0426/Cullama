{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_lhpLaijTc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/customLlama/blob/main/Cullama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Brownwang0426/Cullama.git"
      ],
      "metadata": {
        "id": "HjVzo86SCQvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_A8ONRGk_--"
      },
      "source": [
        "### 0 - Pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqrsEoOuijTe"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub==0.24.5 transformers==4.44.0 trl==0.9.6 peft==0.12.0 bitsandbytes==0.43.3 datasets==2.20.0 openpyxl==3.1.5 xlrd==2.0.1 protobuf==5.27.3 safetensors==0.4.4 faiss-cpu==1.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3VPw7SfijTf"
      },
      "source": [
        "### 1 - Log in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--U3U8mVo_2Q"
      },
      "outputs": [],
      "source": [
        "your_huggingface_finegrained_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eenc8yVijTf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token = your_huggingface_finegrained_token )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIOBVujijTg"
      },
      "source": [
        "### 2 - Download pretrained model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIOBgOno_2S"
      },
      "source": [
        "Select your llama-3 model and the number of layers you want to preserve from huggingface. We support only llama-3 for the present time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHGPoCrzo_2T"
      },
      "outputs": [],
      "source": [
        "your_model_name              = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
        "your_preserved_hidden_layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "if your_preserved_hidden_layers != \"all\":\n",
        "    model_name        = your_model_name\n",
        "    num_hidden_layers = your_preserved_hidden_layers\n",
        "\n",
        "    config            = LlamaConfig.from_pretrained(model_name)\n",
        "    config.num_hidden_layers = num_hidden_layers\n",
        "    new_model         = LlamaForCausalLM(config)\n",
        "    new_state_dict    = {}\n",
        "\n",
        "    old_model         = LlamaForCausalLM.from_pretrained(model_name)\n",
        "    old_state_dict    = old_model.state_dict()\n",
        "\n",
        "    for name, param in new_model.state_dict().items():\n",
        "        if name in old_state_dict:\n",
        "            if 'layers.' in name:\n",
        "                # Check if the layer index is within the range of the new model\n",
        "                original_layer_index = int(name.split('.')[2])\n",
        "                if original_layer_index < config.num_hidden_layers:\n",
        "                    new_state_dict[name] = old_state_dict[name]\n",
        "            else:\n",
        "                new_state_dict[name] = old_state_dict[name]\n",
        "\n",
        "    # Load the filtered state dictionary into the new model\n",
        "    new_model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    model = new_model\n",
        "\n",
        "    tokenizer   = PreTrainedTokenizerFast.from_pretrained(model_name )\n",
        "\n",
        "    # if tokenizer.pad_token is None:\n",
        "    #     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    del old_model\n",
        "\n",
        "else:\n",
        "\n",
        "    model       = LlamaForCausalLM.from_pretrained(your_model_name)\n",
        "    tokenizer   = PreTrainedTokenizerFast.from_pretrained(your_model_name )\n",
        "\n",
        "    # if tokenizer.pad_token is None:\n",
        "    #     tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSdL_Gjv1-"
      },
      "source": [
        "#### *** Check layers, modules, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhvqrp8Bjfug"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKlBha1ijTg"
      },
      "source": [
        "### 3 - Customize your attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPbGLhCgl2Nw"
      },
      "source": [
        "#### 3.1 - Append your attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBtimJjEijTg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MZ-44VdmJ9O"
      },
      "source": [
        "#### 3.2 - Append traditional attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Op8EpDIoYR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SThX_SkomAtl"
      },
      "source": [
        "#### 3.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgPNYHlS420"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2we5nrMyxLrl"
      },
      "source": [
        "#### 3.4 - Replace llama's attn with your attn (consecutive softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWyXQiGxLe-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m30u0pOo_2X"
      },
      "source": [
        "#### 3.5 - No Customization! Only fine-tune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RgrDl9Oo_2X"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHt8IZSNo_2X"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"] # I don't know why these parameters are called \"modules\"... They should be \"parameters\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yhrf8gto_2X"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank decomposition\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    target_modules= target_modules,  # Target modules to apply LoRA\n",
        "    bias=\"none\"  # Bias handling, can be \"none\", \"lora_only\", or \"both\"\n",
        ")\n",
        "\n",
        "model       = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_dqEL4Zo_2X"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqQ7oD3Ho_2X"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nnjoE9kijTh"
      },
      "source": [
        "### 4 - Prep data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGACt7bfIoYS"
      },
      "source": [
        "#### 4.1 Use HF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpt8KXM8o_2Y"
      },
      "outputs": [],
      "source": [
        "your_train_dataset = \"taide/TAIDE-14-tasks\" # Jamie0510/taiwan-law-exam\n",
        "split              = \"train\"\n",
        "sub_set            = ''\n",
        "instruction_col    = 'Prompt'\n",
        "input_col          = 'Input'\n",
        "output_col         = 'Positive Response'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-Q9adqhIoYS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = load_dataset(your_train_dataset, sub_set)[split]\n",
        "original_cols = train_dataset.column_names\n",
        "print(original_cols)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples[instruction_col], examples[input_col] )]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs              , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples[output_col], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns(original_cols)\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFj8QufiIoYS"
      },
      "source": [
        "#### 4.2 Use local data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mRpEYLEo_2Y"
      },
      "outputs": [],
      "source": [
        "your_train_dataset = './Cullama/fintech_qa.xlsx'\n",
        "split              = \"train\"\n",
        "instruction_col    = 'instruction'\n",
        "input_col          = 'input'\n",
        "output_col         = 'output'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PonguRS8IoYS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Read local xlsx\n",
        "import pandas as pd\n",
        "excel_file = pd.ExcelFile(your_train_dataset, engine='openpyxl')\n",
        "for sheet_name in excel_file.sheet_names:\n",
        "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "    df.to_csv(f'./{sheet_name}.csv', index=False)\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset('csv', data_files={'train': f'./{sheet_name}.csv'})[split]\n",
        "original_cols = train_dataset.column_names\n",
        "print(original_cols)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples[instruction_col], examples[input_col])]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs              , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples[output_col], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "train_dataset = train_dataset.remove_columns(original_cols)\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpvJyF9ijTh"
      },
      "source": [
        "### 5 - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqv3Hyhso_2Z"
      },
      "outputs": [],
      "source": [
        "your_learning_rate = 1e-5\n",
        "your_batch_size    = 1\n",
        "your_epoch         = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtcV0l2DijTh"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './result',\n",
        "    learning_rate= your_learning_rate,\n",
        "    per_device_train_batch_size=your_batch_size,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    num_train_epochs=your_epoch,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=  \"no\" # 'epoch',  # Save model every epoch\n",
        "    # save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcT8AgMLIoYT"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmHws9zIoYT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ng5N-o6ijTh"
      },
      "source": [
        "### 6 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOebwXdko_2d"
      },
      "outputs": [],
      "source": [
        "your_user_input = \"川普會當選嗎\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fjMWEeo_2d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "def generate_response(input_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt').to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_length=len(inputs[\"input_ids\"][0]) + 100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.5,\n",
        "            do_sample=True,\n",
        "            top_k=50,   # Optional: for more diverse output\n",
        "            top_p=0.95      # Optional: for more diverse output\n",
        "        )\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = your_user_input\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response.replace(your_user_input, \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0QogOOIoYT"
      },
      "source": [
        "### 7 - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNIBC7gdo_2d"
      },
      "outputs": [],
      "source": [
        "your_test_dataset = \"kigner/ruozhiba-llama3\"\n",
        "split = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPFr9F1CijTh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "test_dataset = load_dataset(your_test_dataset)[split]\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Tokenize the formatted dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs           = [f\"{instruction} {input}\" for instruction, input in zip(examples['instruction'], examples['input'])]\n",
        "    # Tokenize combined inputs\n",
        "    tokenized_inputs = tokenizer(inputs            , padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    # Tokenize outputs (labels)\n",
        "    tokenized_labels = tokenizer(examples['output'], padding=\"max_length\", truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt')\n",
        "    combined_inputs = {\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': tokenized_labels['input_ids']  # Include labels\n",
        "    }\n",
        "    return combined_inputs\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unused columns\n",
        "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
        "\n",
        "# Now train_dataset contains the columns `input_ids`, `attention_mask`, and `labels`\n",
        "print(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "testing_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=1\n",
        ")\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=testing_args,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "trainer_stats = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(trainer_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJvLx7CijTi"
      },
      "source": [
        "### 8 - Save & Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulcghG4bo_2d"
      },
      "outputs": [],
      "source": [
        "your_repo_name  = \"Brownwang0426\"\n",
        "your_model_name = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"\n",
        "your_huggingface_write_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqZX2ggwijTi"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(your_model_name)\n",
        "tokenizer.save_pretrained(your_model_name)\n",
        "model.push_to_hub(your_model_name, token = your_huggingface_write_token) # Online saving\n",
        "tokenizer.push_to_hub(your_model_name, token = your_huggingface_write_token) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcujtpQYijTi"
      },
      "source": [
        "### 9- Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTd-6BUWo_2e"
      },
      "outputs": [],
      "source": [
        "your_model_name              =  \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjHEq5G0ijTi"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model_name = your_model_name # \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BYhx2cqIoYW"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj2xX5qAIoYW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6THnZmPPWj6"
      },
      "source": [
        "### 10 - Customize your attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUiTgRsno6fR"
      },
      "source": [
        "#### 10.1 - Append your attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTYXL0WBP400"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  position_embeddings=position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv7Bfuo6pFd-"
      },
      "source": [
        "#### 10.2 - Append traditional attn to llama's attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UD52W8ZpJz6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q = self.split_heads(self.W_q(Q))\n",
        "            K = self.split_heads(self.W_k(K))\n",
        "            V = self.split_heads(self.W_v(V))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(LlamaAttention):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super().__init__(config, layer_idx)\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.brown_layer = custom_attn(config.hidden_size, 2)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=None,\n",
        "                          use_cache=None,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings=None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.brown_layer(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            # self_attn_weights, present_key_value skipped\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return super().forward(hidden_states       ,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  position_ids=position_ids,\n",
        "                                  past_key_value=past_key_value,\n",
        "                                  output_attentions=output_attentions,\n",
        "                                  use_cache=use_cache,\n",
        "                                  cache_position=cache_position,\n",
        "                                  max_position_embeddings=max_position_embeddings,\n",
        "                                  layer_idx=self.layer_idx,\n",
        "                                  **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOA0BFmCozhP"
      },
      "source": [
        "#### 10.3 - Replace llama's attn with your attn (multi-multi-head attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBonirDSoxzg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_0  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_1  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_2  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "            self.W_q_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o_final  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "            output     = torch.matmul(attn_probs, V)\n",
        "\n",
        "            return output\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q_0 = self.split_heads(self.W_q_0(Q))\n",
        "            K_0 = self.split_heads(self.W_k_0(K))\n",
        "            V_0 = self.split_heads(self.W_v_0(V))\n",
        "            attn_output_0 = self.scaled_dot_product_attention(Q_0, K_0, V_0, mask)\n",
        "            output_0      = self.W_o_0(self.combine_heads(attn_output_0))\n",
        "\n",
        "            Q_1 = self.split_heads(self.W_q_1(Q))\n",
        "            K_1 = self.split_heads(self.W_k_1(K))\n",
        "            V_1 = self.split_heads(self.W_v_1(V))\n",
        "            attn_output_1 = self.scaled_dot_product_attention(Q_1, K_1, V_1, mask)\n",
        "            output_1      = self.W_o_1(self.combine_heads(attn_output_1))\n",
        "\n",
        "            Q_2 = self.split_heads(self.W_q_2(Q))\n",
        "            K_2 = self.split_heads(self.W_k_2(K))\n",
        "            V_2 = self.split_heads(self.W_v_2(V))\n",
        "            attn_output_2 = self.scaled_dot_product_attention(Q_2, K_2, V_2, mask)\n",
        "            output_2      = self.W_o_2(self.combine_heads(attn_output_2))\n",
        "\n",
        "            Q_final = self.split_heads(self.W_q_final(output_0))\n",
        "            K_final = self.split_heads(self.W_k_final(output_1))\n",
        "            V_final = self.split_heads(self.W_v_final(output_2))\n",
        "            attn_output_final = self.scaled_dot_product_attention(Q_final, K_final, V_final, mask)\n",
        "            output_final      = self.W_o_final(self.combine_heads(attn_output_final))\n",
        "\n",
        "            return output_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sHaB6c-yIj1"
      },
      "source": [
        "#### 10.4 - Replace llama's attn with your attn (consecutive softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fK7x-qKyxVW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention\n",
        "\n",
        "customization = True\n",
        "\n",
        "if customization:\n",
        "\n",
        "    class custom_attn(nn.Module):\n",
        "        def __init__(self, d_model, num_heads = 8):\n",
        "            super(custom_attn, self).__init__()\n",
        "\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "            self.bias      = False\n",
        "            self.d_model   = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k       = d_model // num_heads\n",
        "\n",
        "            self.W_q  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_k  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v1 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v2 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v3 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_v4 = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "            self.W_o  = nn.Linear(d_model, d_model, bias=self.bias)\n",
        "\n",
        "\n",
        "        def scaled_dot_product_attention(self, Q, K, V1, V2, V3, mask=None):\n",
        "\n",
        "            attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V1)\n",
        "\n",
        "            attn_scores = torch.matmul(output, V2.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "            if mask is not None:\n",
        "                # llama's values in un-masked poistions are 0\n",
        "                attn_scores += mask\n",
        "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "            output     = torch.matmul(attn_probs, V3)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "        def split_heads(self, x):\n",
        "            batch_size, seq_length, d_model = x.size()\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "            #  (batch_size, seq_length, d_model) - > (batch_size, seq_length, self.num_heads, self.d_k) -> (batch_size, self.num_heads, seq_length, self.d_k)\n",
        "\n",
        "        def combine_heads(self, x):\n",
        "            batch_size, _, seq_length, d_k = x.size()\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        def forward(self, Q, K, V1, V2, V3, mask=None):\n",
        "            # Q    -> (batch_size, seq_length, d_model)\n",
        "            # mask -> (batch_size, 1, seq_length, d_model)\n",
        "            Q  = self.split_heads(self.W_q(Q))\n",
        "            K  = self.split_heads(self.W_k(K))\n",
        "            V1 = self.split_heads(self.W_v1(V1))\n",
        "            V2 = self.split_heads(self.W_v2(V2))\n",
        "            V3 = self.split_heads(self.W_v3(V3))\n",
        "            attn_output = self.scaled_dot_product_attention(Q, K, V1, V2, V3, mask)\n",
        "            output      = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class self_attn(nn.Module):\n",
        "        def __init__(self, config, layer_idx):\n",
        "            super(self_attn, self).__init__()\n",
        "            self.layer_idx   = layer_idx\n",
        "            self.custom_attn = custom_attn(config.hidden_size, 8)\n",
        "\n",
        "        def forward(self, hidden_states,\n",
        "                          attention_mask=None,\n",
        "                          position_ids=None,\n",
        "                          past_key_value=None,\n",
        "                          output_attentions=False,\n",
        "                          use_cache=False,\n",
        "                          cache_position=None,\n",
        "                          max_position_embeddings  =None,\n",
        "                          layer_idx = None,\n",
        "                          **kwargs):\n",
        "\n",
        "            # Apply the custom attention layer\n",
        "            hidden_states = self.custom_attn(hidden_states,hidden_states,hidden_states,hidden_states,hidden_states,attention_mask)\n",
        "            attn_weights = None\n",
        "            past_key_value = None\n",
        "\n",
        "            # Continue with the original LLaMA attention mechanism\n",
        "            return hidden_states, attn_weights, past_key_value\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(model.config.num_hidden_layers):\n",
        "      print(f\"Replacing attention layer: {i}\")\n",
        "      try:\n",
        "        model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "      except:\n",
        "        model.base_model.model.model.layers[i].self_attn = self_attn(model.config, layer_idx=i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3gmnrVqo_2g"
      },
      "source": [
        "#### 10.5 - No Customization! Only fine-tune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqi3qqfvo_2g"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sL0nTrFo_2g"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"] # I don't know why these parameters are called \"modules\"... They should be \"parameters\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3XXjWkvo_2g"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank decomposition\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    target_modules= target_modules,  # Target modules to apply LoRA\n",
        "    bias=\"none\"  # Bias handling, can be \"none\", \"lora_only\", or \"both\"\n",
        ")\n",
        "\n",
        "model       = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XmngNrDP2Zo"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgxI0DkCP4AC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKmDSBFCQZkb"
      },
      "source": [
        "### 11 - Retreive safetensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAT5CCD-o_2g"
      },
      "outputs": [],
      "source": [
        "your_model_name  = \"Brownwang0426/Llama-3-Taiwan-8B-Instruct-to-1B\"\n",
        "your_safetensors = [\"model-00001-of-00002.safetensors\", \"model-00002-of-00002.safetensors\"]\n",
        "your_huggingface_read_token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvAk4n9WQhUS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Replace with your model's repository ID\n",
        "repo_id = your_model_name\n",
        "\n",
        "# Specify the filenames of the SafeTensor files you want to download\n",
        "filenames = your_safetensors\n",
        "\n",
        "# Specify dir\n",
        "custom_cache_dir =  \"./\"\n",
        "\n",
        "# Download each SafeTensor file\n",
        "for filename in filenames:\n",
        "    try:\n",
        "        filepath = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=custom_cache_dir, token = your_huggingface_read_token )\n",
        "    except:\n",
        "        print(f'safetenstor not found: {filename}')\n",
        "\n",
        "state_dict_combined = {}\n",
        "\n",
        "# Iterate over all safetensor files in the directory\n",
        "for filename in os.listdir(os.path.join(custom_cache_dir, \"models--\" + your_model_name.replace(\"/\", \"--\") ,\"blobs\")):\n",
        "    file_path = os.path.join(custom_cache_dir, \"models--\" + your_model_name.replace(\"/\", \"--\") ,\"blobs\", filename)\n",
        "    print(f\"Loading {file_path}...\")\n",
        "\n",
        "    # Load the safetensor file\n",
        "    state_dict = load_file(file_path)\n",
        "\n",
        "    # Merge the state_dict with the merged_state_dict\n",
        "    state_dict_combined.update(state_dict)\n",
        "\n",
        "# Merge the weights from the current SafeTensor\n",
        "for key, tensor in state_dict_combined.items():\n",
        "    if key in model.state_dict():\n",
        "        print(f\"Merging {key}.\")\n",
        "        model.state_dict()[key].copy_(tensor)\n",
        "    else:\n",
        "        print(f\"Warning: {key} not found in the model. Skipping.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz47c6eEQXYp"
      },
      "source": [
        "#### *** Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H_0V6InQhrm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(model.model.layers[0])\n",
        "  print(model.model.layers[0].self_attn.state_dict())\n",
        "except:\n",
        "  print(model.base_model.model.model.layers[0])\n",
        "  print(model.base_model.model.model.layers[0].self_attn.state_dict())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdqZDV7EijTi"
      },
      "source": [
        "### 12 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KGw2cMYo_2h"
      },
      "outputs": [],
      "source": [
        "your_user_input = \"請問為何外匯帳戶容易受到洗錢者的喜愛?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKnSFq0_ijTi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "def generate_response(input_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_attention_mask=True , return_tensors='pt').to(model.device)\n",
        "    # Generate a response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_length=len(inputs[\"input_ids\"][0]) + 100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.5,\n",
        "            do_sample=True,\n",
        "            top_k=50,   # Optional: for more diverse output\n",
        "            top_p=0.95      # Optional: for more diverse output\n",
        "        )\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example user input\n",
        "user_input = your_user_input\n",
        "\n",
        "# Generate and print the response\n",
        "response = generate_response(user_input, model, tokenizer)\n",
        "print(\"Chatbot:\", response.replace(your_user_input, \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13 - Let's play with RAG with this tiny model!"
      ],
      "metadata": {
        "id": "jfo-q_mizw_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqvoePw5OQwx"
      },
      "outputs": [],
      "source": [
        "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Initialize the RAG tokenizer using your LLaMA-3 tokenizer\n",
        "rag_tokenizer =  RagTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the retriever (assuming you're using a specific retriever model or index)\n",
        "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n",
        "\n",
        "# Load your LLaMA-3 model into the RAG model\n",
        "rag_model = RagSequenceForGeneration.from_pretrained(\n",
        "    retriever=rag_retriever,\n",
        "    question_encoder=model,  # Use your LLaMA model here\n",
        "    generator=model  # Again, your LLaMA model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYnZlfg4OQtS"
      },
      "outputs": [],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "\n",
        "input_ids = rag_tokenizer(question, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate answers\n",
        "outputs = rag_model.generate(input_ids)\n",
        "\n",
        "# Decode the output to get the text\n",
        "answers = rag_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy64sEj_OQoK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13DOWEMYOQhP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQbHaYnFijTi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9F3UqXtijTi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}